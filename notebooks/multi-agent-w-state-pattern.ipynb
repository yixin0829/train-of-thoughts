{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Debate with State Pattern\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State Transition with String Matching (Regex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions insituations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "Proponent: Artificial intelligence can analyze vast amounts of data and different perspectives objectively, allowing it to make more rational moral decisions in situations where human bias and emotional conflict lead to stalemate. By leveraging AI, we can ensure that decisions are made based on logic and fairness rather than personal interests or flawed reasoning. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: Allowing AI to make moral decisions undermines human dignity and the complexity of nuanced ethical deliberations; machines lack the emotional intelligence and empathy that are essential in moral reasoning. Human beings, with their capacity for compassion and understanding, should be at the forefront of ethical decision-making, even when there is disagreement. \n",
            "\n",
            "Transition to proponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While emotional intelligence is crucial, it can also cloud judgment and lead to biased outcomes. AI, equipped with diverse ethical frameworks, can help bridge disagreements by providing a neutral analysis that humans can then use as a starting point for meaningful dialogue and resolution. \n",
            "\n",
            "Transition to neutral.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: It's important to consider both sides; AI could help mediate disagreements, but it also raises concerns about autonomy and the quality of moral reasoning. Balancing the strengths of AI with the inherent value of human input might be key in this debate. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: Relying on AI for moral decisions risks creating a scenario where accountability is lost; when machines make mistakes, who is held responsible? Ethical dilemmas often involve deep cultural and emotional contexts that AI cannot fully grasp, making human involvement indispensable. \n",
            "\n",
            "Transition to proponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: Accountability can still be maintained, as the humans programming and overseeing AI systems can be held responsible for the outcomes. Moreover, by utilizing AI to identify potential moral solutions, humans can focus on the intricate emotional and cultural elements of a situation, ultimately leading to more informed and empathetic decisions. \n",
            "\n",
            "Transition to neutral.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: It seems there are valid points on both sides regarding accountability and the limits of AI's understanding of human emotion. Perhaps a collaborative approach, where AI assists but does not solely dictate moral decisions, could lead to a more balanced outcome. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: Collaboration might seem ideal, but it risks deferring responsibility to AI, allowing humans to circumvent difficult ethical choices. The essence of morality involves personal values and societal norms that machines cannot comprehend; ultimately, moral decision-making should remain a distinctly human endeavor. \n",
            "\n",
            "Transition to proponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While personal values and societal norms are vital, humans often struggle to agree on them, leading to gridlock in ethical decision-making. AI can provide valuable insights and options by synthesizing these varied perspectives, facilitating a resolution that might not be otherwise achievable through human debate alone. \n",
            "\n",
            "Transition to neutral.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: The potential for AI to facilitate consensus in moral discussions is intriguing, but it raises questions about the depth of understanding and empathy required in ethical dilemmas. Perhaps a hybrid model that leverages both AI's analytical strength and human emotional insight could help navigate these complex issues. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from abc import ABC, abstractmethod\n",
        "from enum import Enum\n",
        "\n",
        "import litellm\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "MODEL = \"openai/gpt-4o-mini\"\n",
        "MAX_TOKENS = 500\n",
        "\n",
        "# System prompts for the agents\n",
        "PRO_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you agree with: {proposition}.\"\n",
        "    \"Start your response with 'Proponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to opponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you disagree with: {proposition}.\"\n",
        "    \"Start your response with 'Opponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you feel neutral about: {proposition}.\"\n",
        "    \"Start your response with 'Neutral:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to opponent'.\"\n",
        ")\n",
        "\n",
        "\n",
        "class AgentName(Enum):\n",
        "    PROPONENT = \"proponent\"\n",
        "    OPPONENT = \"opponent\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "\n",
        "class DebateContext:\n",
        "    def __init__(\n",
        "        self,\n",
        "        proposition: str,\n",
        "        curr_agent: AgentName,\n",
        "        agents_registry: dict[AgentName, any],\n",
        "    ) -> None:\n",
        "        self.proposition = proposition\n",
        "\n",
        "        self.agents_registry = agents_registry\n",
        "        for agent in self.agents_registry.values():\n",
        "            agent.context = self\n",
        "\n",
        "        self.curr_agent = self.agents_registry[curr_agent.value]\n",
        "        self.messages = []\n",
        "\n",
        "    def run(self):\n",
        "        self.curr_agent.debate()\n",
        "\n",
        "\n",
        "class AgentInterface(ABC):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self._context = None\n",
        "\n",
        "    @property\n",
        "    def messages(self) -> list[dict]:\n",
        "        \"\"\"\n",
        "        The messages history is the system prompt plus the messages from the previous debates.\n",
        "        The system prompt defines the agent's role and its proposition.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": self.instructions}\n",
        "        ] + self.context.messages\n",
        "\n",
        "    @property\n",
        "    def context(self) -> DebateContext:\n",
        "        return self._context\n",
        "\n",
        "    @context.setter\n",
        "    def context(self, context: DebateContext) -> None:\n",
        "        self._context = context\n",
        "\n",
        "    @abstractmethod\n",
        "    def debate(self) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class Agent(AgentInterface):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__(name, instructions)\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        print(f\"{content}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # State transition using string matching (There is a better way to do this using tool calling)\n",
        "        match = re.search(\n",
        "            r\"transition to (proponent|opponent|neutral)\", content, re.IGNORECASE\n",
        "        )\n",
        "        if match:\n",
        "            next_agent_name = match.group(1).lower()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid transition: {content}\")\n",
        "\n",
        "        # Update the messages history to agents a \"short-term memory\"\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "def run_debate(\n",
        "    agents_registry: dict[AgentName, Agent],\n",
        "    proposition: str,\n",
        "    max_turns: int = 10,\n",
        ") -> None:\n",
        "    context = DebateContext(\n",
        "        proposition, curr_agent=AgentName.PROPONENT, agents_registry=agents_registry\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting debate on proposition: {proposition}\\n\")\n",
        "    print(\"=\" * 100)\n",
        "    while len(context.messages) < max_turns:\n",
        "        context.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = (\n",
        "        \"Artificial intelligence should be allowed to make moral decisions in\"\n",
        "        \"situations where humans fail to agree.\"\n",
        "    )\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State Transition with Tool Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"type\": \"function\",\n",
            "  \"function\": {\n",
            "    \"name\": \"handoff\",\n",
            "    \"description\": \"Debate response and transition to the next agent.\",\n",
            "    \"parameters\": {\n",
            "      \"type\": \"object\",\n",
            "      \"properties\": {\n",
            "        \"response\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"The debate response based on the previous debate history (1-2 concise sentences). Start response with the agent's name (e.g. \\\"Proponent: <response>\\\").\"\n",
            "        },\n",
            "        \"next_agent_name\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"The next agent name to transition to. Always transition to a different agent.\",\n",
            "          \"enum\": [\n",
            "            \"proponent\",\n",
            "            \"opponent\",\n",
            "            \"neutral\"\n",
            "          ]\n",
            "        }\n",
            "      },\n",
            "      \"required\": [\n",
            "        \"response\",\n",
            "        \"next_agent_name\"\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "import json\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "def parse_google_docstring(docstring: str) -> dict[str, str]:\n",
        "    if not docstring:\n",
        "        return {}\n",
        "\n",
        "    lines = [line.strip() for line in docstring.split(\"\\n\")]\n",
        "\n",
        "    args_section = False\n",
        "    param_descriptions = {}\n",
        "    current_param = None\n",
        "    current_desc = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.lower().startswith(\"args:\"):\n",
        "            args_section = True\n",
        "            continue\n",
        "\n",
        "        if args_section:\n",
        "            param_match = re.match(r\"^\\s*(\\w+):\\s*(.*)\", line)\n",
        "            if param_match:\n",
        "                if current_param:\n",
        "                    param_descriptions[current_param] = \" \".join(current_desc).strip()\n",
        "\n",
        "                current_param = param_match.group(1)\n",
        "                current_desc = [param_match.group(2).strip()]\n",
        "            elif current_param and line.strip():\n",
        "                current_desc.append(line.strip())\n",
        "\n",
        "    if current_param:\n",
        "        param_descriptions[current_param] = \" \".join(current_desc).strip()\n",
        "\n",
        "    return param_descriptions\n",
        "\n",
        "\n",
        "def function_to_schema(func) -> dict:\n",
        "    type_map = {\n",
        "        str: \"string\",\n",
        "        int: \"integer\",\n",
        "        float: \"number\",\n",
        "        bool: \"boolean\",\n",
        "        list: \"array\",\n",
        "        dict: \"object\",\n",
        "        type(None): \"null\",\n",
        "        Literal: \"string\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        signature = inspect.signature(func)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(\n",
        "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
        "        )\n",
        "\n",
        "    param_descriptions = parse_google_docstring(func.__doc__)\n",
        "\n",
        "    parameters = {}\n",
        "    for param in signature.parameters.values():\n",
        "        try:\n",
        "            param_type = type_map.get(param.annotation, \"string\")\n",
        "        except KeyError as e:\n",
        "            raise KeyError(\n",
        "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
        "            )\n",
        "\n",
        "        param_dict = {\n",
        "            \"type\": param_type,\n",
        "            \"description\": param_descriptions.get(param.name, \"\"),\n",
        "        }\n",
        "\n",
        "        # Add enum field for Literal types\n",
        "        if (\n",
        "            hasattr(param.annotation, \"__origin__\")\n",
        "            and param.annotation.__origin__ == Literal\n",
        "        ):\n",
        "            param_dict[\"enum\"] = list(param.annotation.__args__)\n",
        "        # Add enum field for Enum types - check for Enum inheritance\n",
        "        elif hasattr(param.annotation, \"__members__\") and (\n",
        "            hasattr(param.annotation, \"__enum__\") or issubclass(param.annotation, Enum)\n",
        "            if isinstance(param.annotation, type)\n",
        "            else False\n",
        "        ):\n",
        "            param_dict[\"type\"] = \"string\"\n",
        "            param_dict[\"enum\"] = [\n",
        "                member.value for member in param.annotation.__members__.values()\n",
        "            ]\n",
        "\n",
        "        parameters[param.name] = param_dict\n",
        "\n",
        "    required = [\n",
        "        param.name\n",
        "        for param in signature.parameters.values()\n",
        "        if param.default == inspect._empty\n",
        "    ]\n",
        "\n",
        "    func_description = func.__doc__.split(\"\\n\\n\")[0].strip() if func.__doc__ else \"\"\n",
        "\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": func_description,\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": parameters,\n",
        "                \"required\": required,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "def handoff(response: str, next_agent_name: AgentName) -> None:\n",
        "    \"\"\"\n",
        "    Debate response and transition to the next agent.\n",
        "\n",
        "    Args:\n",
        "        response: The debate response based on the previous debate history (1-2 concise sentences).\n",
        "            Start response with the agent's name (e.g. \"Proponent: <response>\").\n",
        "        next_agent_name: The next agent name to transition to. Always transition to a different agent.\n",
        "\n",
        "    Returns:\n",
        "        Return nothing as this function is used for guiding the LLM to transition to the\n",
        "        next agent only. We will not use the return value.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "schema = function_to_schema(handoff)\n",
        "print(json.dumps(schema, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions insituations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "[Tool call] response: Proponent: Artificial intelligence can analyze vast amounts of data and consider perspectives that h..., next_agent_name: opponent\n",
            "\n",
            "Proponent: Artificial intelligence can analyze vast amounts of data and consider perspectives that humans might overlook, leading to more objective moral decisions in situations where human agreement is elusive.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Opponent: While AI can process data objectively, it lacks the ability to understand human emotions, ..., next_agent_name: neutral\n",
            "\n",
            "Opponent: While AI can process data objectively, it lacks the ability to understand human emotions, context, and subjective experiences that are crucial in moral decision-making. Relying on AI in moral situations could lead to decisions that are cold and devoid of empathy.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Neutral: It's important to consider that AI could help highlight different perspectives but may miss..., next_agent_name: proponent\n",
            "\n",
            "Neutral: It's important to consider that AI could help highlight different perspectives but may miss the emotional nuances required in moral decisions. The interplay between AI's capabilities and human empathy needs further exploration.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Proponent: Indeed, while AI may lack emotional understanding, its ability to process data can identi..., next_agent_name: opponent\n",
            "\n",
            "Proponent: Indeed, while AI may lack emotional understanding, its ability to process data can identify trends and insights that aid in more comprehensive moral considerations, potentially guiding humans toward more informed decisions.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Opponent: Even though AI can provide insights, moral decisions often rely on deeply personal experie..., next_agent_name: neutral\n",
            "\n",
            "Opponent: Even though AI can provide insights, moral decisions often rely on deeply personal experiences and societal values. An AI's inability to grasp these nuances can result in ethically questionable outcomes, undermining the moral fabric of society.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[No tool calling... Randomly transition to a different agent]\n",
            "\n",
            "Neutral: Both perspectives raise valid points. The challenge lies in finding a balance between AI's data-driven insights and the essential human elements of empathy and values in moral decision-making. This debate necessitates further exploration of how AI could be integrated responsibly.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Proponent: The integration of AI in moral decision-making could serve as a support system, enabling ..., next_agent_name: opponent\n",
            "\n",
            "Proponent: The integration of AI in moral decision-making could serve as a support system, enabling humans to reflect on their values and choices, rather than taking away from the human element.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[No tool calling... Randomly transition to a different agent]\n",
            "\n",
            "Opponent: However, using AI as a support system risks normalizing its role in moral decision-making, potentially leading to dependence and a decline in critical thinking and compassion in human decision processes.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[No tool calling... Randomly transition to a different agent]\n",
            "\n",
            "Neutral: It's essential to weigh the risks of dependency on AI against its potential benefits. Exploring methods of collaboration between AI and human judgment could lead to improved moral outcomes while maintaining critical human oversight.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Proponent: Collaboration can enhance human judgment by providing valuable data-driven insights, enco..., next_agent_name: opponent\n",
            "\n",
            "Proponent: Collaboration can enhance human judgment by providing valuable data-driven insights, encouraging deeper ethical discussions and ultimately leading to better moral outcomes.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# System prompts for the agents\n",
        "PRO_AGENT_INSTRUCTIONS = \"\"\"You are a \"Proponent\" agent debating with other agents about a proposition that you agree with: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = \"\"\"You are an \"Opponent\" agent debating with other agents about a proposition that you disagree with: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = \"\"\"You are a \"Neutral\" agent debating with other agents about a proposition that you feel neutral about: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "\n",
        "class Agent(AgentInterface):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__(name, instructions)\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "            tools=[function_to_schema(handoff)],\n",
        "        )\n",
        "\n",
        "        # State transition using tool calling\n",
        "        tool_calls = response.choices[0].message.tool_calls\n",
        "        if tool_calls:\n",
        "            args = json.loads(tool_calls[0].function.arguments)\n",
        "            print(\n",
        "                f\"\\n[Tool call] response: {args['response'][:100]}..., next_agent_name: {args['next_agent_name']}\\n\"\n",
        "            )\n",
        "            content = args[\"response\"]\n",
        "            next_agent_name = args[\"next_agent_name\"]\n",
        "        else:\n",
        "            print(\"\\n[No tool calling... Randomly transition to a different agent]\\n\")\n",
        "            content = response.choices[0].message.content\n",
        "            next_agent_name = random.choice(\n",
        "                [agent for agent in self.context.agents_registry if agent != self]\n",
        "            )\n",
        "\n",
        "        print(f\"{content}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Update the messages history and transition to the next agent\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = (\n",
        "        \"Artificial intelligence should be allowed to make moral decisions in\"\n",
        "        \"situations where humans fail to agree.\"\n",
        "    )\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### State Transition with Structured Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions insituations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "Proponent: Allowing artificial intelligence to make moral decisions in situations where humans fail to agree can lead to more objective and consistent outcomes, free from biases and emotions that often cloud human judgment. This can be especially beneficial in complex scenarios where ethical dilemmas arise and consensus is hard to achieve.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: Relying on artificial intelligence for moral decisions overlooks the deeply contextual and nuanced understanding of human ethics, which machines cannot fully grasp. The risk of dehumanizing our moral frameworks and eroding personal responsibility is too great to justify this approach.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While it's true that AI may lack the emotional nuance of human ethics, its ability to process vast amounts of data can uncover patterns and insights that humans might miss, leading to well-informed decisions. It can serve as a tool to enhance moral reasoning rather than replace it entirely.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: I see valid points on both sides regarding the role AI could play in moral decision-making; the balance between the objectivity of AI and the context-sensitive nature of human ethics is complex. It's a topic worth exploring further to understand both the potential benefits and drawbacks.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: While it is indeed important to explore the complexities of AI in moral decision-making, we should remain cautious about delegating such profound responsibilities to machines that lack human empathy and moral reasoning. The potential for unintended consequences remains a significant concern that must not be overlooked.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: I acknowledge the concern for unintended consequences, however, implementing strict oversight and ethical guidelines can help mitigate these risks while maximizing the benefits of AI in moral decision-making. With careful implementation, we could see a future where AI aids in resolving disagreements with a fair and balanced approach.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: Both the potential for AI to provide fair decision-making and the concerns about oversight and unintended consequences are crucial points in this debate. Finding the right balance between innovation and caution will ultimately determine the success or failure of AI in moral scenarios.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: It's essential to recognize that human morals are not static; they evolve through cultural and social contexts that AI cannot replicate. Automating moral choices may lead to rigid interpretations that could alienate individuals and communities, ultimately resulting in greater division rather than resolution.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While human morals are indeed dynamic and culturally influenced, AI can be programmed to learn from diverse perspectives and adapt its decision-making processes accordingly. This adaptability could enable AI to better reflect evolving human values while providing consistent and unbiased guidance in moral dilemmas.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: I see the potential for AI to adapt and learn from diverse perspectives, but there remains a fundamental question about whether a machine can truly encapsulate the depth of human moral evolution. As we consider implementing AI in moral decision-making, the emphasis should be on continuous dialogue about its ethical implications and societal impacts.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# System prompts for the agents (Same as the 1st example)\n",
        "PRO_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you agree with: {proposition}.\"\n",
        "    \"Start your response with 'Proponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to opponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you disagree with: {proposition}.\"\n",
        "    \"Start your response with 'Opponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you feel neutral about: {proposition}.\"\n",
        "    \"Start your response with 'Neutral:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to opponent'.\"\n",
        ")\n",
        "\n",
        "\n",
        "class DebateResponse(BaseModel):\n",
        "    response: str = Field(\n",
        "        description=\"The debate response based on the previous debate history.\"\n",
        "    )\n",
        "    next_agent_name: AgentName = Field(\n",
        "        description=\"The next agent name to transition to. Always transition to a different agent.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Agent(AgentInterface):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__(name, instructions)\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "            response_format=DebateResponse,\n",
        "        )\n",
        "\n",
        "        # State transition using structured output\n",
        "        parsed_response = DebateResponse.model_validate_json(\n",
        "            response.choices[0].message.content\n",
        "        )\n",
        "        content = parsed_response.response\n",
        "        next_agent_name = parsed_response.next_agent_name.value\n",
        "\n",
        "        print(f\"{content}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Update the messages history and transition to the next agent\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = (\n",
        "        \"Artificial intelligence should be allowed to make moral decisions in\"\n",
        "        \"situations where humans fail to agree.\"\n",
        "    )\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
