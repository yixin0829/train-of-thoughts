{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Debate with State Pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Proponent: I believe that artificial intelligence can provide objective analysis and consistency in moral decision-making, especially in complex situations where human emotions and biases lead to stalemates. By utilizing data-driven approaches, AI can help us arrive at solutions that prioritize the greater good. Transition to opponent.\n",
      "\n",
      "Opponent: But AI lacks the ability to fully understand human experiences and emotions, which are crucial in making moral decisions. Relying on machines to navigate ethical dilemmas risks oversimplifying complex issues and undermining our humanity. Transition to neutral.\n",
      "\n",
      "Neutral: I see both sides of the argument. While AI could offer a consistent framework for moral decisions, it may also miss the nuances of human experience that are essential for ethical considerations. Transition to proponent.\n",
      "\n",
      "Proponent: Exactly, and while AI might miss some nuances, it can synthesize vast amounts of information quicker than any individual human. In scenarios where people are deeply divided, AI can help find common ground based on the ethical principles most aligned with societal values. Transition to opponent.\n",
      "\n",
      "Opponent: However, ethical principles can vary significantly across cultures and contexts, meaning AI's reliance on data can lead to a one-size-fits-all approach that doesn't respect these differences. Trusting AI with moral decisions could inadvertently impose a dominant cultural worldview onto diverse populations. Transition to neutral.\n",
      "\n",
      "Neutral: That's a valid point; the potential for cultural bias in AI decision-making is a significant concern. It raises the question of how we can ensure that AI systems are designed to be inclusive and representative of various ethical perspectives. Transition to proponent.\n",
      "\n",
      "Proponent: Absolutely, this concern highlights the importance of involving diverse groups in the development of AI systems. By incorporating a variety of ethical frameworks, we can create AI that better respects cultural differences while still providing valuable guidance in moral decision-making. Transition to opponent.\n",
      "\n",
      "Opponent: Even with diverse input, AI cannot replace the depth of human judgment shaped by lived experience. Moral decisions often require empathy and understanding that go beyond data; entrusting them to an algorithm risks alienating those affected by the outcomes. Transition to neutral.\n",
      "\n",
      "Neutral: It's true that human empathy plays a crucial role in moral decisions, and while AI can assist with data analysis, it may not fully capture the emotional aspects that influence our choices. This balance between AI and human judgment is a critical factor to consider going forward. Transition to proponent.\n",
      "\n",
      "Proponent: Exactly, and instead of replacing human judgment, AI can act as a tool that enhances our decision-making capability. By providing insights and eliminating biases from the data analysis process, we can make more informed and compassionate choices together. Transition to opponent.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Summary of the debate: The debate centers around the use of artificial intelligence (AI) in moral decision-making, showcasing a dialogue between a proponent and an opponent, with neutral perspectives interjected.\n",
      "\n",
      "**Proponent's Argument:** The advocate for AI argues that it provides objective and consistent analysis in moral decisions, particularly in complex situations where human emotions may cloud judgment. They believe that AI's data-driven approach can facilitate solutions that prioritize the greater good and can help find common ground in divided scenarios.\n",
      "\n",
      "**Opponent's Counterargument:** The opponent counters that AI cannot fully grasp human experiences and emotions, which are vital for ethical decision-making. They express concern that over-reliance on AI could oversimplify complex moral dilemmas and risk undermining human values and empathy.\n",
      "\n",
      "**Neutral Perspective:** The neutral perspective acknowledges the strengths and weaknesses of both sides. While AI can offer a consistent decision-making framework, it might overlook the nuances of human experience and ethical considerations.\n",
      "\n",
      "**Reinforcing Points:** The proponent further argues that AI’s ability to synthesize vast amounts of information can enhance decision-making, with suggestions for including diverse ethical frameworks in AI development to address cultural biases.\n",
      "\n",
      "**Cultural Concerns:** The opponent highlights that ethical principles vary widely across cultures and warns that AI’s data reliance might impose a dominant cultural narrative, risking alienation of diverse populations. \n",
      "\n",
      "**Conclusion on Human Judgment:** Both sides agree on the necessity of human empathy in moral decisions, with the proponent suggesting that AI should complement rather than replace human judgment, serving as a tool to enhance compassionate decision-making. The discussion emphasizes balancing AI capabilities with the irreplaceable depth of human understanding and experience.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import litellm\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"openai/gpt-4o-mini\"\n",
    "MAX_TOKENS = 500\n",
    "\n",
    "# System prompts for the agents\n",
    "PRO_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you agree with: {proposition}. Start your response with 'Proponent:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to opponent' or 'Transition to neutral'.\"\n",
    "\n",
    "CON_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you disagree with: {proposition}. Start your response with 'Opponent:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to neutral'.\"\n",
    "\n",
    "NEUTRAL_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you feel neutral about: {proposition}. Start your response with 'Neutral:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to opponent'.\"\n",
    "\n",
    "\n",
    "class DebateContext:\n",
    "    def __init__(self, proposition: str) -> None:\n",
    "        self.proposition = proposition\n",
    "\n",
    "        # Agent registry (Note: this is equivalent to the state registry in the state pattern)\n",
    "        self.agents = {\n",
    "            \"proponent\": Agent(name=\"Proponent\", instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition), context=self),\n",
    "            \"opponent\": Agent(name=\"Opponent\", instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition), context=self),\n",
    "            \"neutral\": Agent(name=\"Neutral\", instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition), context=self),\n",
    "        }\n",
    "        \n",
    "        self.current_agent = self.agents[\"proponent\"]\n",
    "        self.messages = []\n",
    "    \n",
    "    def run(self):\n",
    "        self.current_agent.debate()\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, instructions: str, context: DebateContext) -> None:\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "        self.context = context\n",
    "\n",
    "    def debate(self) -> str:\n",
    "        # Prepend the system prompt to the messages history\n",
    "        messages = [{\"role\": \"system\", \"content\": self.instructions}] + self.context.messages\n",
    "        response = litellm.completion(\n",
    "            model=MODEL,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            messages=messages,\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        print(f\"{content}\\n\")\n",
    "\n",
    "        # Update the messages history\n",
    "        self.context.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": f\"{content}\"}\n",
    "        )\n",
    "\n",
    "        # State transition using string matching (There is a better way to do this using tool calling)\n",
    "        match = re.search(r\"transition to (proponent|opponent|neutral)\", content, re.IGNORECASE)\n",
    "        if match:\n",
    "            next_agent = match.group(1).lower()\n",
    "            self.context.current_agent = self.context.agents[next_agent]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid transition: {content}\")\n",
    "        \n",
    "        return content\n",
    "\n",
    "\n",
    "def run_debate(proposition: str, max_turns: int = 10):\n",
    "    context = DebateContext(proposition)\n",
    "    \n",
    "    print(f\"\\nStarting debate on proposition: {proposition}\\n\")\n",
    "    print(\"-\" * 100)\n",
    "    # Run the debate\n",
    "    while len(context.messages) < max_turns:\n",
    "        context.run()\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    return context.messages\n",
    "\n",
    "def summarize_debate(messages):\n",
    "    response = litellm.completion(\n",
    "        model=MODEL,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Summarize the debate: {[str(m) for m in messages]}\"}],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    proposition = \"Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\"\n",
    "    messages = run_debate(proposition, max_turns=10)\n",
    "\n",
    "    # Summarize the debate\n",
    "    summary = summarize_debate(messages)\n",
    "    print(f\"\\nSummary of the debate: {summary}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
