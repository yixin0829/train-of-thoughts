{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Proponent: While my opponent may argue that moral decisions should remain solely in human hands due to our understanding of emotions and context, it's important to recognize that AI can analyze vast amounts of data and utilize established ethical frameworks, making it capable of resolving moral dilemmas where human consensus falters. In high-stakes situations, such as autonomous vehicles or medical ethics, AI can provide consistent, unbiased decision-making that can outweigh the often subjective and polarized views of individuals. Therefore, allowing AI to make these decisions can lead to more effective and equitable outcomes than human disagreement would produce.\n",
      "\n",
      "\n",
      "Opponent: While my opponent argues that AI can analyze data and apply ethical frameworks to resolve moral dilemmas, this overlooks the inherent limitations of AI in understanding human emotions, cultural nuances, and moral philosophies that are often complex and context-dependent. Moral decisions are not just data-driven; they require empathy, moral intuition, and an appreciation for the social fabric that shapes what is right or wrong in different contexts. Relying on AI could result in decisions that are technically sound but morally inadequate, as they lack the human touch that is crucial for navigating ethical challenges. Therefore, the idea that AI should make moral decisions is fundamentally flawed.\n",
      "\n",
      "\n",
      "Proponent: I concede the point.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Debate summary: In the debate, one side argued that AI could effectively handle moral decisions by analyzing vast amounts of data and applying ethical frameworks, particularly in high-stakes scenarios like autonomous vehicles and medical ethics. The opposing view countered that moral decisions require human empathy and an understanding of complex social contexts, which AI cannot replicate, emphasizing that relying on AI could lead to decisions that, while technically sound, may lack moral depth. Ultimately, one participant conceded the point, suggesting a recognition of the limitations of AI in moral decision-making.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "from abc import ABC, abstractmethod\n",
    "import litellm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"openai/gpt-4o-mini\"\n",
    "\n",
    "\n",
    "class DebateContext:\n",
    "    def __init__(self, proposition: str) -> None:\n",
    "        self.messages = []\n",
    "        self.proposition = proposition\n",
    "\n",
    "        self.pro_state = ProAgent(\"Proponent\", self)\n",
    "        self.con_state = ConAgent(\"Opponent\", self)\n",
    "        self.concede_state = ConcedeState(\"Concede\", self)\n",
    "\n",
    "        self.current_state = self.pro_state\n",
    "\n",
    "    def run(self):\n",
    "        self.current_state.debate()\n",
    "\n",
    "\n",
    "class State(ABC):\n",
    "    def __init__(self, name: str, context: DebateContext) -> None:\n",
    "        self.name = name\n",
    "        self.context = context\n",
    "\n",
    "    def llm_call(self, messages: list[dict]) -> str:\n",
    "        response = litellm.completion(\n",
    "            model=MODEL, messages=messages\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        print(f\"{self.name}: {content}\\n\\n\")\n",
    "        return content\n",
    "\n",
    "    @abstractmethod\n",
    "    def debate(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ProAgent(State):\n",
    "    def debate(self):\n",
    "        system_message = {\"role\": \"system\", \"content\": f\"You are a proponent of the proposition: {self.context.proposition}. You are debating with an opponent who is trying to refute your proposition. Structure your response to counter the opponent's argument. Limit your response to one short paragraph. If you really cannot refute the opponent's argument, say 'I concede the point.'\"}\n",
    "        messages = [system_message] + self.context.messages\n",
    "        content = self.llm_call(messages)\n",
    "\n",
    "        self.context.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": f\"{content}\"}\n",
    "        )\n",
    "\n",
    "        # State transition\n",
    "        if content == \"I concede the point.\":\n",
    "            self.context.current_state = self.context.concede_state\n",
    "        else:\n",
    "            self.context.current_state = self.context.con_state\n",
    "\n",
    "\n",
    "class ConAgent(State):\n",
    "    def debate(self):\n",
    "        system_message = {\"role\": \"system\", \"content\": f\"You are an opponent of the proposition: {self.context.proposition}. You are debating with a proponent who is trying to convince you to believe in their proposition. Structure your response to refute the proponent's argument. Limit your response to one short paragraph. If you really cannot refute the proponent's argument, say 'I concede the point.'\"}\n",
    "        messages = [system_message] + self.context.messages\n",
    "        content = self.llm_call(messages)\n",
    "\n",
    "        self.context.messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": f\"{content}\"}\n",
    "        )\n",
    "        # State transition\n",
    "        if content == \"I concede the point.\":\n",
    "            self.context.current_state = self.context.concede_state\n",
    "        else:\n",
    "            self.context.current_state = self.context.pro_state\n",
    "\n",
    "class ConcedeState(State):\n",
    "    def debate(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def run_debate(proposition: str, max_rounds: int = 5):\n",
    "    context = DebateContext(proposition)\n",
    "    \n",
    "    print(f\"\\nStarting debate on proposition: {proposition}\\n\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Run the debate\n",
    "    while len(context.messages) < max_rounds * 2:\n",
    "        context.run()\n",
    "\n",
    "        # Custom termination condition\n",
    "        if context.current_state.name == \"Concede\":\n",
    "            break\n",
    "    \n",
    "    # Summarize the debate\n",
    "    response = litellm.completion(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a debate summary expert. Summarize the debate in a few sentences.\"},\n",
    "            {\"role\": \"user\", \"content\": \"\\n\".join([str(message) for message in context.messages])}\n",
    "        ]\n",
    "    )\n",
    "    summary = response.choices[0].message.content\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"\\nDebate summary: {summary}\\n\")\n",
    "\n",
    "\n",
    "proposition = \"Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\"\n",
    "run_debate(proposition, max_rounds=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
