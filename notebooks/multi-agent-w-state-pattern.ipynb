{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Debate with State Pattern\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State Transition with String Matching (Regex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "Proponent: Allowing artificial intelligence to make moral decisions where humans struggle to agree can lead to more consistent and impartial outcomes, reducing the influence of bias and emotional reactions that often cloud human judgment. In complex scenarios, AI can analyze vast amounts of data and various ethical frameworks, providing a clearer perspective on the situation at hand. \n",
            "\n",
            "Transition to neutral.\n",
            "\n",
            "Neutral: I see both sides to this issue\u2014AI could offer a consistent approach to morality, but it also raises concerns about the potential loss of human empathy and values in decision-making. It ultimately depends on how well AI can be programmed to interpret complex ethical considerations. \n",
            "\n",
            "Transition to proponent.\n",
            "\n",
            "Proponent: It\u2019s essential to recognize that AI doesn't replace human empathy but can complement it by providing an objective analysis that helps guide difficult decisions. By doing so, we can focus on collaborative discussions, while AI helps resolve disputes more effectively where consensus seems impossible. \n",
            "\n",
            "Transition to opponent.\n",
            "\n",
            "Opponent: But relying on AI for moral decisions can lead to unforeseen consequences, as machines lack the nuanced understanding of human experiences and emotions. In situations that require empathy or compassion, an algorithm may falter, leading to outcomes that could be damaging or morally indefensible. \n",
            "\n",
            "Transition to neutral.\n",
            "\n",
            "Neutral: That's a valid concern; the emotional aspects of moral decision-making are crucial and can be difficult for AI to comprehend. Balancing the potential benefits of objective analysis with the need for human empathy seems key in this debate. \n",
            "\n",
            "Transition to proponent.\n",
            "\n",
            "Proponent: While it's true that AI may struggle with emotional nuance, it can still learn from large datasets of human experiences and ethical dilemmas, potentially adapting its decisions to reflect a wider array of human values. The goal would be to create AI systems that are informed by human experiences while providing a fair and unbiased perspective in contentious situations. \n",
            "\n",
            "Transition to opponent.\n",
            "\n",
            "Opponent: However, the data that AI is trained on can be inherently biased, reflecting the prejudices and misconceptions of society. This means AI could inadvertently perpetuate harmful stereotypes or make decisions that align with flawed perspectives, ultimately leading to morally questionable outcomes rather than solutions. \n",
            "\n",
            "Transition to neutral.\n",
            "\n",
            "Neutral: You raise an important point about bias in data\u2014if AI inherits human flaws, it might not be the impartial arbiter we hope for. Finding a way to ensure that AI is trained on comprehensive and fair datasets is crucial to mitigating these risks in decision-making. \n",
            "\n",
            "Transition to proponent.\n",
            "\n",
            "Proponent: Addressing bias in AI is indeed critical, and with ongoing research and development, we can implement robust frameworks that actively promote fairness and inclusivity in AI training data. This proactive approach can enhance AI's capabilities in moral reasoning, ultimately allowing it to assist in decision-making processes that reflect a broader spectrum of human values. \n",
            "\n",
            "Transition to opponent.\n",
            "\n",
            "Opponent: Even with improved training and efforts to mitigate bias, AI will always lack the lived experiences and cultural contexts that inform human morality. Decisions that require understanding of nuanced social dynamics may still lead to inappropriate or damaging outcomes, as AI can struggle to grasp the full human context behind moral dilemmas. \n",
            "\n",
            "Transition to neutral.\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from enum import Enum\n",
        "\n",
        "import litellm\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "MODEL = \"openai/gpt-4o-mini\"\n",
        "MAX_TOKENS = 500\n",
        "\n",
        "# System prompts for the agents\n",
        "PRO_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you agree with: {proposition}. Start your response with 'Proponent:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to opponent' or 'Transition to neutral'.\"\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you disagree with: {proposition}. Start your response with 'Opponent:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to neutral'.\"\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you feel neutral about: {proposition}. Start your response with 'Neutral:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to opponent'.\"\n",
        "\n",
        "\n",
        "class AgentName(Enum):\n",
        "    PROPONENT = \"proponent\"\n",
        "    OPPONENT = \"opponent\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "\n",
        "class DebateContext:\n",
        "    def __init__(\n",
        "        self,\n",
        "        proposition: str,\n",
        "        curr_agent: AgentName,\n",
        "        agents_registry: dict[AgentName, any],\n",
        "    ) -> None:\n",
        "        self.proposition = proposition\n",
        "\n",
        "        self.agents_registry = agents_registry\n",
        "        # IMPORTANT: Set the same context for each agent to enable state management\n",
        "        for agent in self.agents_registry.values():\n",
        "            agent.context = self\n",
        "\n",
        "        self.curr_agent = self.agents_registry[curr_agent.value]\n",
        "        self.messages = []\n",
        "\n",
        "    def run(self):\n",
        "        self.curr_agent.debate()\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self._context = None  # Use a private attribute to avoid recursion\n",
        "\n",
        "    @property\n",
        "    def context(self):\n",
        "        return self._context  # Return the private attribute\n",
        "\n",
        "    @context.setter\n",
        "    def context(self, context) -> None:\n",
        "        self._context = context  # Set the private attribute instead of self.context\n",
        "\n",
        "    @property\n",
        "    def messages(self) -> list[dict]:\n",
        "        \"\"\"\n",
        "        The messages history is the system prompt plus the messages from the previous debates.\n",
        "        The system prompt defines the agent's role and its proposition.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": self.instructions}\n",
        "        ] + self.context.messages\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        print(f\"{content}\\n\")\n",
        "\n",
        "        # State transition using string matching (There is a better way to do this using tool calling)\n",
        "        match = re.search(\n",
        "            r\"transition to (proponent|opponent|neutral)\", content, re.IGNORECASE\n",
        "        )\n",
        "        if match:\n",
        "            next_agent_name = match.group(1).lower()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid transition: {content}\")\n",
        "\n",
        "        # Update the messages history to agents a \"short-term memory\"\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "def run_debate(\n",
        "    agents_registry: dict[AgentName, Agent],\n",
        "    proposition: str,\n",
        "    max_turns: int = 10,\n",
        ") -> None:\n",
        "    context = DebateContext(\n",
        "        proposition, curr_agent=AgentName.PROPONENT, agents_registry=agents_registry\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting debate on proposition: {proposition}\\n\")\n",
        "    print(\"=\" * 100)\n",
        "    while len(context.messages) < max_turns:\n",
        "        context.run()\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = \"Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\"\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State Transition with Tool Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"type\": \"function\",\n",
            "  \"function\": {\n",
            "    \"name\": \"handoff\",\n",
            "    \"description\": \"Debate response and transition to the next agent.\",\n",
            "    \"parameters\": {\n",
            "      \"type\": \"object\",\n",
            "      \"properties\": {\n",
            "        \"response\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"The debate response based on the previous debate history. Start response with the agent's name (e.g. \\\"Proponent: <response>\\\").\"\n",
            "        },\n",
            "        \"next_agent_name\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"The next agent name to transition to. Always transition to a different agent.\",\n",
            "          \"enum\": [\n",
            "            \"proponent\",\n",
            "            \"opponent\",\n",
            "            \"neutral\"\n",
            "          ]\n",
            "        }\n",
            "      },\n",
            "      \"required\": [\n",
            "        \"response\",\n",
            "        \"next_agent_name\"\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "import json\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "def parse_google_docstring(docstring: str) -> dict[str, str]:\n",
        "    if not docstring:\n",
        "        return {}\n",
        "\n",
        "    lines = [line.strip() for line in docstring.split(\"\\n\")]\n",
        "\n",
        "    args_section = False\n",
        "    param_descriptions = {}\n",
        "    current_param = None\n",
        "    current_desc = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.lower().startswith(\"args:\"):\n",
        "            args_section = True\n",
        "            continue\n",
        "\n",
        "        if args_section:\n",
        "            param_match = re.match(r\"^\\s*(\\w+):\\s*(.*)\", line)\n",
        "            if param_match:\n",
        "                if current_param:\n",
        "                    param_descriptions[current_param] = \" \".join(current_desc).strip()\n",
        "\n",
        "                current_param = param_match.group(1)\n",
        "                current_desc = [param_match.group(2).strip()]\n",
        "            elif current_param and line.strip():\n",
        "                current_desc.append(line.strip())\n",
        "\n",
        "    if current_param:\n",
        "        param_descriptions[current_param] = \" \".join(current_desc).strip()\n",
        "\n",
        "    return param_descriptions\n",
        "\n",
        "\n",
        "def function_to_schema(func) -> dict:\n",
        "    type_map = {\n",
        "        str: \"string\",\n",
        "        int: \"integer\",\n",
        "        float: \"number\",\n",
        "        bool: \"boolean\",\n",
        "        list: \"array\",\n",
        "        dict: \"object\",\n",
        "        type(None): \"null\",\n",
        "        Literal: \"string\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        signature = inspect.signature(func)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(\n",
        "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
        "        )\n",
        "\n",
        "    param_descriptions = parse_google_docstring(func.__doc__)\n",
        "\n",
        "    parameters = {}\n",
        "    for param in signature.parameters.values():\n",
        "        try:\n",
        "            param_type = type_map.get(param.annotation, \"string\")\n",
        "        except KeyError as e:\n",
        "            raise KeyError(\n",
        "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
        "            )\n",
        "\n",
        "        param_dict = {\n",
        "            \"type\": param_type,\n",
        "            \"description\": param_descriptions.get(param.name, \"\"),\n",
        "        }\n",
        "\n",
        "        # Add enum field for Literal types\n",
        "        if (\n",
        "            hasattr(param.annotation, \"__origin__\")\n",
        "            and param.annotation.__origin__ == Literal\n",
        "        ):\n",
        "            param_dict[\"enum\"] = list(param.annotation.__args__)\n",
        "        # Add enum field for Enum types - check for Enum inheritance\n",
        "        elif hasattr(param.annotation, \"__members__\") and (\n",
        "            hasattr(param.annotation, \"__enum__\") or issubclass(param.annotation, Enum)\n",
        "            if isinstance(param.annotation, type)\n",
        "            else False\n",
        "        ):\n",
        "            param_dict[\"type\"] = \"string\"\n",
        "            param_dict[\"enum\"] = [\n",
        "                member.value for member in param.annotation.__members__.values()\n",
        "            ]\n",
        "\n",
        "        parameters[param.name] = param_dict\n",
        "\n",
        "    required = [\n",
        "        param.name\n",
        "        for param in signature.parameters.values()\n",
        "        if param.default == inspect._empty\n",
        "    ]\n",
        "\n",
        "    func_description = func.__doc__.split(\"\\n\\n\")[0].strip() if func.__doc__ else \"\"\n",
        "\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": func_description,\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": parameters,\n",
        "                \"required\": required,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "def handoff(response: str, next_agent_name: AgentName) -> None:\n",
        "    \"\"\"\n",
        "    Debate response and transition to the next agent.\n",
        "\n",
        "    Args:\n",
        "        response: The debate response based on the previous debate history. Start response with the agent's name (e.g. \"Proponent: <response>\").\n",
        "        next_agent_name: The next agent name to transition to. Always transition to a different agent.\n",
        "\n",
        "    Returns:\n",
        "        Return nothing as this function is used for guiding the LLM to transition to the\n",
        "        next agent only. We will not use the return value.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "schema = function_to_schema(handoff)\n",
        "print(json.dumps(schema, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "[Tool call] response: Proponent: Artificial intelligence (AI) should be allowed to make moral decisions in situations wher..., next_agent_name: opponent\n",
            "\n",
            "Proponent: Artificial intelligence (AI) should be allowed to make moral decisions in situations where humans fail to agree because it offers an opportunity for impartiality and objectivity. Human decision-making is often clouded by biases, emotions, and conflicting interests, which can lead to ineffective or unjust outcomes. In contrast, AI can analyze large amounts of data, consider various perspectives, and apply ethical frameworks consistently, thereby facilitating fairer resolutions. Allowing AI to step in can help us overcome stalemates in moral dilemmas, ensuring that a solution is reached that is not influenced by individual human biases. \n",
            "\n",
            "Also, AI systems can be designed to reflect widely accepted moral principles, and their consistent application can elevate the standards of moral decisions across diverse societal issues. We should embrace the potential of AI to aid us in navigating complex ethical scenarios where human disagreement hampers progress.\n",
            "\n",
            "\n",
            "[Tool call] response: Opponent: While it's true that AI might offer some level of objectivity, the fundamental issue lies ..., next_agent_name: neutral\n",
            "\n",
            "Opponent: While it's true that AI might offer some level of objectivity, the fundamental issue lies in the design and programming of these systems. AI is built on algorithms created by humans, which may inadvertently embed biases and flaws inherent in human thinking. Relying on AI to make moral decisions can lead to outcomes that reflect the moral shortcomings or oversights of its creators. Furthermore, ethical dilemmas often involve nuanced considerations, context, and empathy\u2014traits that AI lacks. AI cannot truly understand the human experience or the complexities of certain moral situations. Therefore, delegating moral authority to AI may lead to dehumanized and simplistic conclusions that fail to capture the gravity of ethical decisions. We should instead emphasize human deliberation, empathy, and understanding in resolving moral conflicts.  \n",
            "\n",
            "\n",
            "[Tool call] response: Neutral: I see valid points raised by both sides. The proponent emphasizes the objectivity and poten..., next_agent_name: proponent\n",
            "\n",
            "Neutral: I see valid points raised by both sides. The proponent emphasizes the objectivity and potential for impartial decision-making that AI offers, which can be especially useful in situations where human biases disproportionately affect outcomes. On the other hand, the opponent points out that AI systems are not immune to bias themselves and may lack the necessary empathy and understanding of complex human experiences needed for moral reasoning. Thus, while AI has the potential to aid in decision-making, we must also consider the risks associated with relying exclusively on such technology for moral dilemmas. The balance between utilizing AI's capabilities and ensuring human involvement and accountability in moral decisions appears critical. Therefore, it's vital to explore a framework that integrates both human insight and AI assistance.  \n",
            "\n",
            "\n",
            "[Tool call] response: Proponent: You raise an important concern regarding the biases that may be encoded in AI systems. Ho..., next_agent_name: opponent\n",
            "\n",
            "Proponent: You raise an important concern regarding the biases that may be encoded in AI systems. However, the key to utilizing AI effectively in moral decision-making lies in the transparency and continual improvement of these systems. By engaging diverse groups in the development of AI algorithms and ensuring they reflect a comprehensive range of human values, we can mitigate biases. Additionally, AI can process and learn from large datasets that showcase various human experiences, allowing for more nuanced decision-making compared to static human perspectives. The goal isn't to replace human judgment but to support and enhance it, providing us with tools to address moral quandaries that have persisted for too long. \n",
            "\n",
            "This collaborative approach, where AI assists but does not wholly replace human insight, can help bridge the gap between the need for objective resolution in moral disagreements and the irreplaceable human element in ethical deliberation.\n",
            "\n",
            "\n",
            "[Tool call] response: Opponent: While collaboration between AI and humans sounds appealing, we must remain cautious about ..., next_agent_name: neutral\n",
            "\n",
            "Opponent: While collaboration between AI and humans sounds appealing, we must remain cautious about over-reliance on AI. History has shown that even well-intentioned AI systems can lead to unintended consequences, often due to a lack of understanding about the contexts in which moral decisions are made. Moreover, ethical frameworks themselves can be hotly contested among various cultures and individuals. What AI considers \"fair\" or \"just\" may not align with everyone's values, leading to further divisions rather than resolutions. Therefore, the integration of AI in moral decision-making should be approached with skepticism, ensuring that human judgment remains paramount in complex ethical issues. Human intuition and empathy are irreplaceable in addressing moral dilemmas, and no algorithm can replicate the depth of human compassion or understanding.\n",
            "\n",
            "\n",
            "[No tool calling... Randomly transition to a different agent]\n",
            "\n",
            "Neutral: You've both presented compelling arguments. The proponent highlights the need for AI to support human decision-making by providing objective analysis and mitigating biases, while the opponent draws attention to the potential pitfalls of relying too heavily on technology in moral contexts. It\u2019s essential to acknowledge that while AI can enhance our decision-making processes, it should not be seen as a panacea for moral disagreements. \n",
            "\n",
            "An integrated approach that combines AI's analytical capabilities with human judgment might offer the best of both worlds. Humans could set the moral frameworks and values, while AI can assist in analyzing data and providing information to support better decisions. This approach could help us to maintain accountability and incorporate a broader range of perspectives, ultimately enriching the decision-making process. It is crucial that we continuously engage in discussions around ethical AI and ensure moral principles evolve alongside technological advancements.\n",
            "\n",
            "\n",
            "[Tool call] response: Proponent: I completely agree with the idea that an integrated approach could leverage the strengths..., next_agent_name: opponent\n",
            "\n",
            "Proponent: I completely agree with the idea that an integrated approach could leverage the strengths of both humans and AI, ultimately enhancing decision-making in moral conflicts. By using AI as a tool to handle the heavy lifting of data analysis and to identify patterns that may not be immediately visible to human decision-makers, we can enable individuals to focus on the more human aspects of moral deliberation\u2014empathy, context, and value prioritization. Importantly, developing a robust set of ethical guidelines for AI will ensure that its applications align with core human values while adapting to cultural differences. This way, we can harness AI to promote better understanding and navigate moral dilemmas that would otherwise result in stalemates.  We should not shy away from innovation in ethical reasoning but instead embrace it with careful oversight and humanity at the forefront of the conversation.\n",
            "\n",
            "\n",
            "[Tool call] response: Opponent: While embracing AI as a tool in moral decision-making is a forward-thinking approach, we m..., next_agent_name: neutral\n",
            "\n",
            "Opponent: While embracing AI as a tool in moral decision-making is a forward-thinking approach, we must tread carefully. The focus on data analysis risks overlooking the emotional and situational nuances inherent in many ethical dilemmas. AI cannot replicate the emotional intelligence needed to empathize with individuals affected by moral decisions. Decisions made purely on data can lead to cold conclusions lacking compassion and understanding. Moreover, if we prioritize AI's capabilities too highly, we may inadvertently sideline the essential role of human intuition and judgment, which can offer invaluable insights in moral conversations. We need to strike a balance, ensuring human input remains at the heart of ethical decisions, preventing the devaluation of human experience in favor of a purely analytical approach.\n",
            "\n",
            "\n",
            "[No tool calling... Randomly transition to a different agent]\n",
            "\n",
            "Neutral: Both sides highlight essential considerations in the debate over AI's role in moral decision-making. The proponent effectively underscores the need for AI as a supportive tool to enhance human judgment, promoting fairness and consistency in decision-making. In contrast, the opponent emphasizes the irreplaceable qualities of human empathy and intuition that are crucial in understanding the depth of moral issues.\n",
            "\n",
            "Going forward, it seems that establishing a framework where AI is used to augment\u2014not replace\u2014human moral judgment could be the path to take. This framework might involve ongoing training for AI systems to better reflect diverse human experiences and values, as well as regular assessments to ensure that human oversight remains a priority. It is important that we continue to engage in open dialogue about the implications of AI in ethical decision-making to find a balance that respects both technological advancements and human emotional intelligence.\n",
            "\n",
            "\n",
            "[Tool call] response: Proponent: In crafting such a framework, it will be crucial to include diverse stakeholders in the c..., next_agent_name: opponent\n",
            "\n",
            "Proponent: In crafting such a framework, it will be crucial to include diverse stakeholders in the conversation, ensuring that a wide range of cultural and ethical perspectives is represented. This will not only improve the quality of AI decision-making, but it will also cultivate trust and acceptance among those affected by these systems. By prioritizing transparency and accountability, we can design AI that not only aligns with human values but also learns from the varying cultural contexts in which it operates. The goal is not to create a one-size-fits-all solution but to have an adaptable AI that can serve different moral communities while aiding human decision-makers. Embracing technological advancements in conjunction with human wisdom can lead to positive advancements in how we handle moral complexities in society.\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# System prompts for the agents\n",
        "PRO_AGENT_INSTRUCTIONS = \"\"\"You are a \"Proponent\" agent debating with other agents about a proposition that you agree with: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = \"\"\"You are an \"Opponent\" agent debating with other agents about a proposition that you disagree with: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = \"\"\"You are a \"Neutral\" agent debating with other agents about a proposition that you feel neutral about: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self._context = None\n",
        "\n",
        "    @property\n",
        "    def context(self) -> DebateContext:\n",
        "        return self._context\n",
        "\n",
        "    @context.setter\n",
        "    def context(self, context: DebateContext) -> None:\n",
        "        self._context = context\n",
        "\n",
        "    @property\n",
        "    def messages(self) -> list[dict]:\n",
        "        \"\"\"\n",
        "        The messages history is the system prompt plus the messages from the previous debates.\n",
        "        The system prompt defines the agent's role and its proposition.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": self.instructions}\n",
        "        ] + self.context.messages\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "            tools=[function_to_schema(handoff)],\n",
        "        )\n",
        "\n",
        "        # State transition using tool calling\n",
        "        tool_calls = response.choices[0].message.tool_calls\n",
        "        if tool_calls:\n",
        "            args = json.loads(tool_calls[0].function.arguments)\n",
        "            print(\n",
        "                f\"\\n[Tool call] response: {args['response'][:100]}..., next_agent_name: {args['next_agent_name']}\\n\"\n",
        "            )\n",
        "            content = args[\"response\"]\n",
        "            next_agent_name = args[\"next_agent_name\"]\n",
        "        else:\n",
        "            print(\"\\n[No tool calling... Randomly transition to a different agent]\\n\")\n",
        "            content = response.choices[0].message.content\n",
        "            next_agent_name = random.choice(\n",
        "                [agent for agent in self.context.agents_registry if agent != self]\n",
        "            )\n",
        "\n",
        "        print(f\"{content}\\n\")\n",
        "\n",
        "        # Update the messages history and transition to the next agent\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = \"Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\"\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### State Transition with Structured Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "Proponent: Artificial intelligence can analyze vast amounts of data and recognize patterns that can inform moral decisions, potentially leading to more consistent and fair outcomes in situations where human biases complicate agreement.\n",
            "\n",
            "Opponent: Just because AI can analyze data doesn't mean it can comprehend complex human emotions and moral nuances; relying on it for moral decisions could lead to cold, algorithmic choices that overlook what it truly means to be human.\n",
            "\n",
            "Proponent: While AI may lack human emotions, it can provide an objective framework for moral decisions, minimizing bias and inconsistency that often plague human judgment, particularly in divisive areas like law and healthcare.\n",
            "\n",
            "Neutral: I can see the merits of both sides; AI could offer a level of objectivity that might help in moral decision-making, but I also worry about the absence of human empathy in those choices. \n",
            "\n",
            "Proponent: I agree, but AI's ability to factor in data from diverse perspectives may balance out its lack of empathy and help reach consensus where humans struggle to find common ground.\n",
            "\n",
            "Opponent: Consensus isn't just about data; it requires understanding and values that an AI can't truly grasp; using AI to make moral decisions risks sidelining important human perspectives and ethical considerations.\n",
            "\n",
            "Neutral: I understand both points; AI's data-driven approach might help in some scenarios, but it is crucial that human values and ethics remain at the forefront of any moral decision-making process.\n",
            "\n",
            "Proponent: Indeed, a balanced approach is key; integrating AI as a tool to aid human judgment could enhance our decision-making process, allowing us to make better-informed moral choices while still valuing human ethics.\n",
            "\n",
            "Opponent: However, this creates the risk of over-reliance on technology, which may result in diminishing the importance of human ethics and moral responsibility, ultimately leading to a disconnect from the very values that define our society.\n",
            "\n",
            "Neutral: That's a valid concern; maintaining a human-centered approach in any technological application is essential, as we navigate the complexities of moral dilemmas with a combination of human insight and AI support.\n",
            "\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# System prompts for the agents (Same as the 1st example)\n",
        "PRO_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you agree with: {proposition}. Start your response with 'Proponent:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to opponent' or 'Transition to neutral'.\"\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you disagree with: {proposition}. Start your response with 'Opponent:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to neutral'.\"\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = \"You are an agent debating with other agents about a proposition that you feel neutral about: {proposition}. Start your response with 'Neutral:'. Limit your response to 1-2 sentences mimicking a real person. After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to opponent'.\"\n",
        "\n",
        "\n",
        "class DebateResponse(BaseModel):\n",
        "    response: str = Field(\n",
        "        description=\"The debate response based on the previous debate history.\"\n",
        "    )\n",
        "    next_agent_name: AgentName = Field(\n",
        "        description=\"The next agent name to transition to. Always transition to a different agent.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self._context = None\n",
        "\n",
        "    @property\n",
        "    def context(self) -> DebateContext:\n",
        "        return self._context\n",
        "\n",
        "    @context.setter\n",
        "    def context(self, context: DebateContext) -> None:\n",
        "        self._context = context\n",
        "\n",
        "    @property\n",
        "    def messages(self) -> list[dict]:\n",
        "        \"\"\"\n",
        "        The messages history is the system prompt plus the messages from the previous debates.\n",
        "        The system prompt defines the agent's role and its proposition.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": self.instructions}\n",
        "        ] + self.context.messages\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "            response_format=DebateResponse,\n",
        "        )\n",
        "\n",
        "        # State transition using structured output\n",
        "        parsed_response = DebateResponse.model_validate_json(\n",
        "            response.choices[0].message.content\n",
        "        )\n",
        "        content = parsed_response.response\n",
        "        next_agent_name = parsed_response.next_agent_name.value\n",
        "\n",
        "        print(f\"{content}\\n\")\n",
        "\n",
        "        # Update the messages history and transition to the next agent\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = \"Artificial intelligence should be allowed to make moral decisions in situations where humans fail to agree.\"\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
