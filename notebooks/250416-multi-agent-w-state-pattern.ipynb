{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Agent Debate with State Pattern from Scratch\n",
        "\n",
        "AI agents have reshaped our feeds in 2025. In the latest [Forbes 2025 AI 50 List](https://www.forbes.com/lists/ai50/), Sequoia Capital observed that [AI assistants are increasingly moving from chatbots to workflow completion](https://www.sequoiacap.com/article/ai-50-2025/). The trend is clear: in the coming years, these systems will gain more agency, enabling us to delegate more \"long-horizon tasks\" to them. [A research conducted by METR](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) shows that AI\u2019s ability to complete extended tasks doubles every seven months, with current capabilities around one hour.\n",
        "\n",
        "With this ongoing trend, a suite of agent orchestration frameworks has emerged, including LangGraph by LangChain, OpenAI Agents SDK (formerly Swarm), CrewAI etc. These tools provide abstractions and pre-built workflows that streamline rapid prototyping and production deployment. **Yet despite their convenience, I believe it's possible to build most production-grade agentic systems using vanilla LLM API calls and sound software engineering principles**\u2014a similar opinion can also be found in Anthropic's engineering blog [*Building Effective Agents*](https://www.anthropic.com/engineering/building-effective-agents). You will gain a much deeper understanding of how these systems work and iterate quickly with customizability by investing the upfront effort to implement some of the LLM agent patterns from the bottom up.\n",
        "\n",
        "In this notebook, we'll implement a multi-agent debate system from scratch using only LLM API calls and [the state pattern](https://en.wikipedia.org/wiki/State_pattern) from software engineering. Our system orchestrates a debate between proponent, opponent, and neutral agents, with self-directed transitions between once they've responded. No orchestration frameworks like LangGraph or AutoGen, just API calls and software engineering.\n",
        "\n",
        "This isn't to say conventional frameworks aren't useful. They excel at rapid development and give a sense of what's possible. My goal in writing this notebook is merely to show people, including myself, how to implement these systems from scratch.\n",
        "\n",
        "## What Is an Agent?\n",
        "\n",
        "An agent is characterized by its ability to act and reason. It acts on its environment and reasons based on its observations and prior knowledge. An LLM agent leverages a large language model to both reason and act. ChatGPT is a basic agent: depending on the user query, it chooses to generate images, search the internet, or retrieve relevant facts from its memory. Beyond this, techniques like function calling and Claude\u2019s MCP have made it possible for LLMs to interact with external environments\u2014whether the internet, databases, or third-party APIs.\n",
        "\n",
        "Chip Huyen summarized this well in [her blog about agents](https://huyenchip.com/2025/01/07/agents.html). Current agents generally perform three types of actions:\n",
        "\n",
        "1. **Knowledge augmentation** (e.g., web search, vector retrieval, structured queries using text-to-SQL or Cypher)  \n",
        "2. **Capability extension** (e.g., SQL execution, code interpretation)  \n",
        "3. **Write actions** (e.g., generating artifacts like tables, charts, or code)  \n",
        "\n",
        "## What Are Multi-Agent Systems?\n",
        "\n",
        "It's important to differentiate between workflows and autonomous multi-agent systems. Many Y Combinator\u2013backed companies like Harvey favor workflows for control, where steps are predefined by developers. Autonomous multi-agent systems, by contrast, resemble conversations in which agents play roles, perform actions, and hand off information to one another. There is also a hybrid approach in which high-level steps are predefined by humans with autonomous agents embedded within each step.\n",
        "\n",
        "In our multi-agent debate example, each LLM agent both debates with other agents (reasoning) and transitions to the next speaker (action). In particular, we explore three different ways to implement the state transition (i.e. how the agent decides which agent to transition to next).\n",
        "\n",
        "You could extend these agents to search the internet for facts supporting their viewpoints, learn advanced debating techniques, or use coding tools to generate on-the-fly charts and tables. The possibilities are endless. Ultimately, perhaps you will realize that you can build all these capabilities using vanilla LLM API calls and software engineering skills.\n",
        "\n",
        "Let's dive into the implementation.\n",
        "\n",
        "![alt text](../assets/250416-intro.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State Transition with String Matching (Regex)\n",
        "\n",
        "We first define the agent names and system prompt template for each agent here. The system prompts will dictate the behaviour of different agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from abc import ABC, abstractmethod\n",
        "from enum import Enum\n",
        "\n",
        "import litellm\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "MODEL = \"openai/gpt-4o-mini\"\n",
        "MAX_TOKENS = 500\n",
        "\n",
        "\n",
        "class AgentName(Enum):\n",
        "    PROPONENT = \"proponent\"\n",
        "    OPPONENT = \"opponent\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "\n",
        "\n",
        "# System prompts for the agents\n",
        "PRO_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you agree with: {proposition}.\"\n",
        "    \"Start your response with 'Proponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to opponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you disagree with: {proposition}.\"\n",
        "    \"Start your response with 'Opponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you feel neutral about: {proposition}.\"\n",
        "    \"Start your response with 'Neutral:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to opponent'.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`DebateContext` class will take in the proposition, the current agent, and the agents registry. It will also keep track of the messages history and the current agent. Messages history will provide the \"short-term memory\" for the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DebateContext:\n",
        "    def __init__(\n",
        "        self,\n",
        "        proposition: str,\n",
        "        curr_agent: AgentName,\n",
        "        agents_registry: dict[AgentName, any],\n",
        "    ) -> None:\n",
        "        self.proposition = proposition\n",
        "\n",
        "        self.agents_registry = agents_registry\n",
        "        for agent in self.agents_registry.values():\n",
        "            agent.context = self\n",
        "\n",
        "        self.curr_agent = self.agents_registry[curr_agent.value]\n",
        "        self.messages = []\n",
        "\n",
        "    def run(self):\n",
        "        self.curr_agent.debate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the `AgentInterface` class that all agents will inherit from. It will have a `debate` method that will be implemented by each agent. The `messages` property will return the agent-specific system prompt plus the messages from the previous debates. The `context` property will return the current debate context. Notice that each agent and the `DebateContext` will have a reference to each other. This bidirectional reference is important for the state transition (see line `self.context.curr_agent = self.context.agents_registry[next_agent_name]` in the `Agent` class)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions insituations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "Proponent: Allowing artificial intelligence to make moral decisions in situations where humans cannot reach a consensus can lead to more objective outcomes, free from emotional bias and personal conflicts that often hinder human judgment. AI can process vast amounts of information and ethical frameworks to arrive at reasoned conclusions that are consistent and fair. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: While AI may handle data objectively, it lacks the nuanced understanding of human emotions and cultural values that are essential in moral decision-making. Relying on AI for such critical judgments risks imposing a one-size-fits-all solution that could disregard the very complexities of human ethics. \n",
            "\n",
            "Transition to proponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While it's true that AI lacks human emotions, its ability to analyze complex ethical frameworks and data can help identify rational solutions that might escape human discourse. By providing a consistent approach, AI can help mediate disputes, ensuring that decisions are based on logic rather than divisive emotions. \n",
            "\n",
            "Transition to neutral.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: I understand both sides of the argument. On one hand, AI can offer consistency and objectivity, but on the other hand, it may overlook the emotional and cultural intricacies of moral dilemmas which are vital for informed decision-making. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: Exactly, and using AI to make these moral decisions may lead to a lack of accountability. Unlike humans, AI can't be held responsible for the consequences of its decisions, ultimately leaving individuals and communities vulnerable to unchecked technological power that might not reflect their values. \n",
            "\n",
            "Transition to proponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While accountability is a valid concern, we can design AI systems with clearly defined ethical guidelines and human oversight to ensure that they align with community values. Furthermore, AI can help reveal biases in human decision-making, leading to discussions that improve accountability and ethical clarity in the long run. \n",
            "\n",
            "Transition to neutral.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: It's clear that both accountability and oversight are crucial factors when considering AI's role in moral decisions. Though AI can offer insights and consistency, it is essential to approach its integration thoughtfully to address ethical concerns while maximizing its potential benefits. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: However, there remains a fundamental issue regarding the transparency of AI decision-making processes. Many AI algorithms operate as \"black boxes,\" making it difficult for humans to understand how decisions are made, which undermines trust and could perpetuate systemic biases without proper checks. \n",
            "\n",
            "Transition to proponent.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: Transparency in AI is indeed critical, which is why ongoing research focuses on developing explainable AI models that make their reasoning clear. By prioritizing transparency and allowing for human oversight, we can create systems that not only enhance moral decision-making but also earn public trust. \n",
            "\n",
            "Transition to neutral.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: The discussion around transparency in AI highlights the importance of developing understandable systems that can foster trust. Balancing these technical advancements with the ethical implications remains a challenge that requires careful consideration from all stakeholders involved. \n",
            "\n",
            "Transition to opponent.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class AgentInterface(ABC):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.instructions = instructions\n",
        "        self._context = None\n",
        "\n",
        "    @property\n",
        "    def messages(self) -> list[dict]:\n",
        "        \"\"\"\n",
        "        The messages history is the system prompt plus the messages from the previous debates.\n",
        "        The system prompt defines the agent's role and its proposition.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": self.instructions}\n",
        "        ] + self.context.messages\n",
        "\n",
        "    @property\n",
        "    def context(self) -> DebateContext:\n",
        "        return self._context\n",
        "\n",
        "    @context.setter\n",
        "    def context(self, context: DebateContext) -> None:\n",
        "        self._context = context\n",
        "\n",
        "    @abstractmethod\n",
        "    def debate(self) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class Agent(AgentInterface):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__(name, instructions)\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "        )\n",
        "        content = response.choices[0].message.content\n",
        "        print(f\"{content}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # State transition using string matching (There is a better way to do this using tool calling)\n",
        "        match = re.search(\n",
        "            r\"transition to (proponent|opponent|neutral)\", content, re.IGNORECASE\n",
        "        )\n",
        "        if match:\n",
        "            next_agent_name = match.group(1).lower()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid transition: {content}\")\n",
        "\n",
        "        # Update the messages history to agents a \"short-term memory\"\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "def run_debate(\n",
        "    agents_registry: dict[AgentName, Agent],\n",
        "    proposition: str,\n",
        "    max_turns: int = 10,\n",
        ") -> None:\n",
        "    context = DebateContext(\n",
        "        proposition, curr_agent=AgentName.PROPONENT, agents_registry=agents_registry\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting debate on proposition: {proposition}\\n\")\n",
        "    print(\"=\" * 100)\n",
        "    while len(context.messages) < max_turns:\n",
        "        context.run()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = (\n",
        "        \"Artificial intelligence should be allowed to make moral decisions in\"\n",
        "        \"situations where humans fail to agree.\"\n",
        "    )\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State Transition with Tool Calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "String matching is just one way to implement the state transition. We can also use tool calling to transition to the next agent. We define a helper function `function_to_schema` that converts a function to a OpenAI tool schema based on the function's docstring.\n",
        "\n",
        "This is inspired by [OpenAI's Cookbook](https://cookbook.openai.com/examples/orchestrating_agents)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"type\": \"function\",\n",
            "  \"function\": {\n",
            "    \"name\": \"handoff\",\n",
            "    \"description\": \"Debate response and transition to the next agent.\",\n",
            "    \"parameters\": {\n",
            "      \"type\": \"object\",\n",
            "      \"properties\": {\n",
            "        \"response\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"The debate response based on the previous debate history (1-2 concise sentences). Start response with the agent's name (e.g. \\\"Proponent: <response>\\\").\"\n",
            "        },\n",
            "        \"next_agent_name\": {\n",
            "          \"type\": \"string\",\n",
            "          \"description\": \"The next agent name to transition to. Always transition to a different agent.\",\n",
            "          \"enum\": [\n",
            "            \"proponent\",\n",
            "            \"opponent\",\n",
            "            \"neutral\"\n",
            "          ]\n",
            "        }\n",
            "      },\n",
            "      \"required\": [\n",
            "        \"response\",\n",
            "        \"next_agent_name\"\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import inspect\n",
        "import json\n",
        "from typing import Literal\n",
        "\n",
        "\n",
        "def parse_google_docstring(docstring: str) -> dict[str, str]:\n",
        "    if not docstring:\n",
        "        return {}\n",
        "\n",
        "    lines = [line.strip() for line in docstring.split(\"\\n\")]\n",
        "\n",
        "    args_section = False\n",
        "    param_descriptions = {}\n",
        "    current_param = None\n",
        "    current_desc = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.lower().startswith(\"args:\"):\n",
        "            args_section = True\n",
        "            continue\n",
        "\n",
        "        if args_section:\n",
        "            param_match = re.match(r\"^\\s*(\\w+):\\s*(.*)\", line)\n",
        "            if param_match:\n",
        "                if current_param:\n",
        "                    param_descriptions[current_param] = \" \".join(current_desc).strip()\n",
        "\n",
        "                current_param = param_match.group(1)\n",
        "                current_desc = [param_match.group(2).strip()]\n",
        "            elif current_param and line.strip():\n",
        "                current_desc.append(line.strip())\n",
        "\n",
        "    if current_param:\n",
        "        param_descriptions[current_param] = \" \".join(current_desc).strip()\n",
        "\n",
        "    return param_descriptions\n",
        "\n",
        "\n",
        "def function_to_schema(func) -> dict:\n",
        "    type_map = {\n",
        "        str: \"string\",\n",
        "        int: \"integer\",\n",
        "        float: \"number\",\n",
        "        bool: \"boolean\",\n",
        "        list: \"array\",\n",
        "        dict: \"object\",\n",
        "        type(None): \"null\",\n",
        "        Literal: \"string\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        signature = inspect.signature(func)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(\n",
        "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
        "        )\n",
        "\n",
        "    param_descriptions = parse_google_docstring(func.__doc__)\n",
        "\n",
        "    parameters = {}\n",
        "    for param in signature.parameters.values():\n",
        "        try:\n",
        "            param_type = type_map.get(param.annotation, \"string\")\n",
        "        except KeyError as e:\n",
        "            raise KeyError(\n",
        "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
        "            )\n",
        "\n",
        "        param_dict = {\n",
        "            \"type\": param_type,\n",
        "            \"description\": param_descriptions.get(param.name, \"\"),\n",
        "        }\n",
        "\n",
        "        # Add enum field for Literal types\n",
        "        if (\n",
        "            hasattr(param.annotation, \"__origin__\")\n",
        "            and param.annotation.__origin__ == Literal\n",
        "        ):\n",
        "            param_dict[\"enum\"] = list(param.annotation.__args__)\n",
        "        # Add enum field for Enum types - check for Enum inheritance\n",
        "        elif hasattr(param.annotation, \"__members__\") and (\n",
        "            hasattr(param.annotation, \"__enum__\") or issubclass(param.annotation, Enum)\n",
        "            if isinstance(param.annotation, type)\n",
        "            else False\n",
        "        ):\n",
        "            param_dict[\"type\"] = \"string\"\n",
        "            param_dict[\"enum\"] = [\n",
        "                member.value for member in param.annotation.__members__.values()\n",
        "            ]\n",
        "\n",
        "        parameters[param.name] = param_dict\n",
        "\n",
        "    required = [\n",
        "        param.name\n",
        "        for param in signature.parameters.values()\n",
        "        if param.default == inspect._empty\n",
        "    ]\n",
        "\n",
        "    func_description = func.__doc__.split(\"\\n\\n\")[0].strip() if func.__doc__ else \"\"\n",
        "\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": func_description,\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": parameters,\n",
        "                \"required\": required,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "\n",
        "def handoff(response: str, next_agent_name: AgentName) -> None:\n",
        "    \"\"\"\n",
        "    Debate response and transition to the next agent.\n",
        "\n",
        "    Args:\n",
        "        response: The debate response based on the previous debate history (1-2 concise sentences).\n",
        "            Start response with the agent's name (e.g. \"Proponent: <response>\").\n",
        "        next_agent_name: The next agent name to transition to. Always transition to a different agent.\n",
        "\n",
        "    Returns:\n",
        "        Return nothing as this function is used for guiding the LLM to transition to the\n",
        "        next agent only. We will not use the return value.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "schema = function_to_schema(handoff)\n",
        "print(json.dumps(schema, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Majority of the code is the same as the previous example. The only difference is that we use tool calling to transition to the next agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions insituations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "[Tool call] response: Proponent: Allowing artificial intelligence to make moral decisions in situations where humans fail ..., next_agent_name: opponent\n",
            "\n",
            "Proponent: Allowing artificial intelligence to make moral decisions in situations where humans fail to agree can help ensure objective and consistent outcomes, free from personal biases and emotional influences. This allows for a more rational approach to ethics and morality.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Opponent: While the idea of AI making moral decisions seems appealing, it poses severe risks, as AI ..., next_agent_name: neutral\n",
            "\n",
            "Opponent: While the idea of AI making moral decisions seems appealing, it poses severe risks, as AI lacks true understanding, empathy, and the ability to grasp context\u2014factors essential for moral reasoning. Dependence on AI could lead to harmful consequences and moral dilemmas that require human discretion.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Neutral: Both sides present valid points. While AI can provide consistency, it lacks the nuanced und..., next_agent_name: proponent\n",
            "\n",
            "Neutral: Both sides present valid points. While AI can provide consistency, it lacks the nuanced understanding that human moral reasoning entails. It\u2019s crucial to strike a balance between leveraging AI's capabilities and maintaining human oversight in ethical decisions.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Proponent: While AI may lack emotional understanding, its ability to analyze data and recognize patt..., next_agent_name: opponent\n",
            "\n",
            "Proponent: While AI may lack emotional understanding, its ability to analyze data and recognize patterns can lead to more informed decisions that reflect societal values. AI can support human judgment rather than replace it entirely, enhancing decision-making processes.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Opponent: Enhancing decision-making processes with AI raises concerns about accountability and trans..., next_agent_name: neutral\n",
            "\n",
            "Opponent: Enhancing decision-making processes with AI raises concerns about accountability and transparency. AI algorithms can amplify existing biases or create new ones, leading to unfair moral outcomes. Human beings, with their capacity for empathy and moral reasoning, must remain central in ethical decision-making processes.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[No tool calling... Randomly transition to a different agent]\n",
            "\n",
            "Neutral: The debate highlights the complexities of integrating AI into moral decision-making. It\u2019s vital to implement safeguards and ensure that AI operates under human supervision to mitigate biases and maintain ethical standards. A hybrid model may be the most effective approach.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Proponent: A hybrid model that incorporates both AI and human values can enhance moral decision-maki..., next_agent_name: opponent\n",
            "\n",
            "Proponent: A hybrid model that incorporates both AI and human values can enhance moral decision-making. AI can analyze and present options based on data, while humans can provide contextual understanding and empathy, ensuring ethical considerations are met.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[No tool calling... Randomly transition to a different agent]\n",
            "\n",
            "Opponent: However, relying on AI to analyze data can still pose a danger if the data itself is flawed or biased. The risk of technology reinforcing societal inequalities and moral blind spots is significant. Human oversight is essential, as AI may not be equipped to handle the complexities of real-world ethics.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Neutral: It's clear that both integrating AI and ensuring human oversight are necessary for sound et..., next_agent_name: proponent\n",
            "\n",
            "Neutral: It's clear that both integrating AI and ensuring human oversight are necessary for sound ethical decision-making. However, constant monitoring is required to prevent biases from slipping in. It\u2019s important to create transparent systems in AI to build trust and accountability.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Tool call] response: Proponent: Constant monitoring and transparency in AI systems can minimize the risk of biases affect..., next_agent_name: opponent\n",
            "\n",
            "Proponent: Constant monitoring and transparency in AI systems can minimize the risk of biases affecting decision-making. By openly sharing the data and algorithms used, we can build trust and demonstrate AI\u2019s role as a helpful tool in moral decision-making.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "# System prompts for the agents\n",
        "PRO_AGENT_INSTRUCTIONS = \"\"\"You are a \"Proponent\" agent debating with other agents about a proposition that you agree with: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = \"\"\"You are an \"Opponent\" agent debating with other agents about a proposition that you disagree with: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = \"\"\"You are a \"Neutral\" agent debating with other agents about a proposition that you feel neutral about: {proposition}.\n",
        "Always call `handoff(response, next_agent_name)` function to debate and then transition to the next agent.\"\"\"\n",
        "\n",
        "\n",
        "class Agent(AgentInterface):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__(name, instructions)\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "            tools=[function_to_schema(handoff)],\n",
        "        )\n",
        "\n",
        "        # State transition using tool calling\n",
        "        tool_calls = response.choices[0].message.tool_calls\n",
        "        if tool_calls:\n",
        "            args = json.loads(tool_calls[0].function.arguments)\n",
        "            print(\n",
        "                f\"\\n[Tool call] response: {args['response'][:100]}..., next_agent_name: {args['next_agent_name']}\\n\"\n",
        "            )\n",
        "            content = args[\"response\"]\n",
        "            next_agent_name = args[\"next_agent_name\"]\n",
        "        else:\n",
        "            print(\"\\n[No tool calling... Randomly transition to a different agent]\\n\")\n",
        "            content = response.choices[0].message.content\n",
        "            next_agent_name = random.choice(\n",
        "                [agent for agent in self.context.agents_registry if agent != self]\n",
        "            )\n",
        "\n",
        "        print(f\"{content}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Update the messages history and transition to the next agent\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = (\n",
        "        \"Artificial intelligence should be allowed to make moral decisions in\"\n",
        "        \"situations where humans fail to agree.\"\n",
        "    )\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### State Transition with Structured Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we're not actually calling any tools, we can use structured output to transition to the next agent. We define a Pydantic model `DebateResponse` that will be used to parse the response from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting debate on proposition: Artificial intelligence should be allowed to make moral decisions insituations where humans fail to agree.\n",
            "\n",
            "====================================================================================================\n",
            "Proponent: We often see situations where moral disagreements lead to gridlock and inaction, which can have dire consequences; AI can analyze vast amounts of data and provide objective solutions, making decisions that prioritize overall well-being. By incorporating consistent moral frameworks, AI can help us break the deadlock in moral decision-making where humans fail to reach consensus.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: Allowing AI to make moral decisions undermines the complexity of human emotions and ethical considerations, which are necessary for true moral reasoning. We cannot rely on algorithms to capture the nuances of human experience and values, especially in sensitive situations where empathy and personal context are crucial.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While human emotions are vital, they can also lead to biased and inconsistent decisions; AI can help standardize moral reasoning by applying ethical principles without being influenced by personal biases or emotions, thus facilitating fairer outcomes. Embracing AI in such capacities could enhance our decision-making processes rather than diminish them.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: While AI could provide analytical support to moral decision-making, it also lacks the deep understanding of human values and context that often guide these decisions, making it a double-edged sword. I think it's essential to weigh the advantages and disadvantages before fully endorsing AI's role in moral dilemmas.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: The reliance on AI could lead to the erosion of personal accountability and moral responsibility; if we outsource ethical decisions to machines, we risk relinquishing our own ethical agency. Additionally, the algorithms used by AI can reflect biases from the data they were trained on, perpetuating injustice rather than resolving it.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: It's important to recognize that AI can be designed to minimize bias, utilizing diverse data and inclusive ethical frameworks, which can ultimately lead to more equitable outcomes than human decision-making processes marred by personal biases. By viewing AI as a complement to human judgment rather than a replacement, we can enhance our collective moral decision-making capabilities.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: AI certainly holds potential for assisting in moral frameworks by providing broader data analysis, yet it still requires human oversight to ensure that the subtleties of ethical dilemmas are respected. Balancing AI's analytical strengths with human emotional intelligence seems crucial for effective moral decision-making.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Opponent: We must not forget that moral dilemmas often require compassion, intuition, and deep understanding\u2014qualities that AI simply cannot replicate. Trusting machines in such critical areas risks diminishing the human spirit and our capacity for empathy, leading to decisions that might be technically efficient but morally bankrupt.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Proponent: While compassion and intuition are undeniably important, they can also cloud judgment and lead to inconsistent outcomes; AI can enhance the decision-making process by ensuring that all relevant factors are considered without the emotional turmoil that often skews human reasoning. By integrating AI as a tool alongside human insight, we can achieve a more balanced and just approach to complex moral issues.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Neutral: It seems that while AI can provide a structured approach to moral reasoning, its lack of true understanding of human emotions raises legitimate concerns. Ultimately, establishing a framework for AI involvement in moral decision-making that incorporates human insight may strike a needed balance.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# System prompts for the agents (Same as the 1st example)\n",
        "PRO_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you agree with: {proposition}.\"\n",
        "    \"Start your response with 'Proponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to opponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "CON_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you disagree with: {proposition}.\"\n",
        "    \"Start your response with 'Opponent:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to neutral'.\"\n",
        ")\n",
        "\n",
        "NEUTRAL_AGENT_INSTRUCTIONS = (\n",
        "    \"You are an agent debating with other agents about a proposition that you feel neutral about: {proposition}.\"\n",
        "    \"Start your response with 'Neutral:'. Limit your response to 1-2 sentences mimicking a real person.\"\n",
        "    \"After you respond, you can transition to the next agent by saying either 'Transition to proponent' or 'Transition to opponent'.\"\n",
        ")\n",
        "\n",
        "\n",
        "class DebateResponse(BaseModel):\n",
        "    response: str = Field(\n",
        "        description=\"The debate response based on the previous debate history.\"\n",
        "    )\n",
        "    next_agent_name: AgentName = Field(\n",
        "        description=\"The next agent name to transition to. Always transition to a different agent.\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Agent(AgentInterface):\n",
        "    def __init__(self, name: str, instructions: str) -> None:\n",
        "        super().__init__(name, instructions)\n",
        "\n",
        "    def debate(self) -> str:\n",
        "        response = litellm.completion(\n",
        "            model=MODEL,\n",
        "            max_tokens=MAX_TOKENS,\n",
        "            messages=self.messages,\n",
        "            response_format=DebateResponse,\n",
        "        )\n",
        "\n",
        "        # State transition using structured output\n",
        "        parsed_response = DebateResponse.model_validate_json(\n",
        "            response.choices[0].message.content\n",
        "        )\n",
        "        content = parsed_response.response\n",
        "        next_agent_name = parsed_response.next_agent_name.value\n",
        "\n",
        "        print(f\"{content}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        # Update the messages history and transition to the next agent\n",
        "        self.context.messages.append({\"role\": \"assistant\", \"content\": f\"{content}\"})\n",
        "        self.context.curr_agent = self.context.agents_registry[next_agent_name]\n",
        "\n",
        "        return content\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    proposition = (\n",
        "        \"Artificial intelligence should be allowed to make moral decisions in\"\n",
        "        \"situations where humans fail to agree.\"\n",
        "    )\n",
        "    agents_registry = {\n",
        "        AgentName.PROPONENT.value: Agent(\n",
        "            name=\"Proponent\",\n",
        "            instructions=PRO_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.OPPONENT.value: Agent(\n",
        "            name=\"Opponent\",\n",
        "            instructions=CON_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "        AgentName.NEUTRAL.value: Agent(\n",
        "            name=\"Neutral\",\n",
        "            instructions=NEUTRAL_AGENT_INSTRUCTIONS.format(proposition=proposition),\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    run_debate(agents_registry, proposition, max_turns=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we implemented a multi-agent debate system from scratch using only LLM API calls and the state pattern from software engineering. We explored three different ways to implement the state transition: string matching, tool calling, and structured output.\n",
        "\n",
        "In general, I find that structured output is the most elegant way to implement the state transition. String matching will be the most flexible way [without sacrificing model's reasoning ability](https://arxiv.org/abs/2408.02442). While tool calling is sensitive to how prompt is written, sometimes the model does not call the tool even when it is instructed to do so.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
